defmodule ReadmeGraphExample do
  def run do
    readme_map = %{
      "CVE-2023-44487" => "# CVE-2023-44487\nBasic vulnerability scanning to see if web servers may be vulnerable to CVE-2023-44487\n\nThis tool checks to see if a website is vulnerable to CVE-2023-44487 completely non-invasively.\n\n1. The tool checks if a web server accepts HTTP/2 requests without downgrading them\n2. If the web server accepts and does not downgrade HTTP/2 requests the tool attempts to open a connection stream and subsequently reset it\n3. If the web server accepts the creation and resetting of a connection stream then the server is definitely vulnerable, if it only accepts HTTP/2 requests but the stream connection fails it may be vulnerable if the server-side capabilities are enabled.\n\nTo run,\n\n    $ python3 -m pip install -r requirements.txt\n\n    $ python3 cve202344487.py -i input_urls.txt -o output_results.csv\n\nYou can also specify an HTTP proxy to proxy all the requests through with the `--proxy` flag\n\n    $ python3 cve202344487.py -i input_urls.txt -o output_results.csv --proxy http://proxysite.com:1234\n\nThe script outputs a CSV file with the following columns\n\n- Timestamp: a timestamp of the request\n- Source Internal IP: The internal IP address of the host sending the HTTP requests\n- Source External IP: The external IP address of the host sending the HTTP requests\n- URL: The URL being scanned\n- Vulnerability Status: \"VULNERABLE\"/\"LIKELY\"/\"POSSIBLE\"/\"SAFE\"/\"ERROR\"\n- Error/Downgrade Version: The error or the version the HTTP server downgrades the request to\n\n*Note: \"Vulnerable\" in this context means that it is confirmed that an attacker can reset the a stream connection without issue, it does not take into account implementation-specific or volume-based detections*",
      "django-tui" => "# django-tui\n\nInspect and run Django Commands in a text-based user interface (TUI), built with [Textual](https://github.com/Textualize/textual) & [Trogon](https://github.com/Textualize/trogon).\n\n------\n\n## Features\n\n- Run Django commands in a text-based user interface (TUI)\n- Inspect Django configs, models, and more\n\n## Installation\n\n```console\npip install django-tui\n```\n\nAdd `\"django_tui\"` to your `INSTALLED_APPS` setting in `settings.py` like this:\n\n\n```python\nINSTALLED_APPS = [\n    ...,\n    \"django_tui\",\n]\n```\n\nNow you can run the TUI with:\n\n```console\npython manage.py tui\n```",
      "easier-docker" => "# easier-docker\n\n## Repository Introduction\nThis is based on [docker-py](https://github.com/docker/docker-py?tab=readme-ov-file) which makes it easier to run your program in docker.\nConfigure your container image information more easily in python, allowing the container in docker to execute the configured program you want to execute.\n\n## Usage\n Please check config parameters in [Docker SDK for Python](https://docker-py.readthedocs.io/en/stable/containers.html)\n\n### Use examples in code\n```bash\npython example.py\n```\n```python\n# example.py\nimport os\n\nfrom easierdocker import EasierDocker\n\nif __name__ == '__main__':\n    host_script = os.path.dirname(os.path.abspath(__file__))\n    container_script = '/path/to/container'\n    config = {\n        'image': 'python:3.9',\n        'name': 'python_test',\n        'volumes': {\n            f'{host_script}': {'bind': container_script, 'mode': 'rw'}\n        },\n        'detach': True,\n        'command': [\"sh\", \"-c\", f'cd {container_script} &&'\n                                'python docker_example.py'],\n    }\n    easier_docker = EasierDocker(config)\n    easier_docker.start()\n    \"\"\"\n    >>> 2023-12-29 15:17:31,901 - INFO - easier-docker ==> Network id: [13c5a6cb0137], name: [host]\n    >>> 2023-12-29 15:17:31,901 - INFO - easier-docker ==> Network id: [27d6b39aeef6], name: [none]\n    >>> 2023-12-29 15:17:31,901 - INFO - easier-docker ==> Network id: [2c9ae2fbfe9d], name: [bridge]\n    >>> 2023-12-29 15:17:31,901 - INFO - easier-docker ==> Network: [bridge] is found locally...\n    >>> 2023-12-29 15:17:31,901 - INFO - easier-docker ==> Find docker image: [python:3.9] locally...\n    >>> 2023-12-29 15:17:31,906 - INFO - easier-docker ==> Image: [python:3.9] is found locally\n    >>> 2023-12-29 15:17:31,906 - INFO - easier-docker ==> Find docker container: [python_test] locally...\n    >>> 2023-12-29 15:17:31,910 - INFO - easier-docker ==> ContainerNotFound: [python_test], it will be created\n    >>> 2023-12-29 15:17:34,217 - INFO - easier-docker ==> Container name: [python_test] is running\n    >>> 2023-12-29 15:17:34,217 - INFO - easier-docker ==> Container id: [fd7fad6e9995] is running\n    >>> 2023-12-29 15:17:34,217 - INFO - easier-docker ==> Container ip address: [172.17.0.2]\n    >>> 2023-12-29 15:17:34,217 - INFO - easier-docker ==> Successfully container is running and be created at 2023-12-29T07:17:31.912747785Z\n    \"\"\"\n```\nThe content of docker_example.py is\n```python\n# docker_example.py\ndef main():\n    import logging\n    import time\n    for i in range(1, 101):\n        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n        logging.info(f'sleep 30s, times:{i}')\n        time.sleep(30)\n\n\nif __name__ == '__main__':\n    main()\n\n```\n\n### Run directly from configuration file\nCurrently supports type of file: _yml_, _yaml_, _json_\n```bash\neasier-docker -c config.yaml\n```\nThe content of config.yaml is\n```yaml\n# config.yaml\nimage: python:3.9\nname: python_test\nvolumes:\n  /Users/admin/data/code_project/easier-docker/example:\n    bind: /path/to/container\n    mode: rw\ndetach: true\ncommand:\n  - sh\n  - -c\n  - cd /path/to/container && python docker_example.py\n      \n    # >>> 2023-12-29 15:08:58,703 - INFO - easier-docker ==> config =\n    # >>>  {\n    # >>>     \"image\": \"python:3.9\",\n    # >>>     \"name\": \"python_test\",\n    # >>>     \"volumes\": {\n    # >>>         \"D:\\\\code-project\\\\EasierDocker\\\\example\": {\n    # >>>             \"bind\": \"/path/to/container\",\n    # >>>             \"mode\": \"rw\"\n    # >>>         }\n    # >>>     },\n    # >>>     \"detach\": true,\n    # >>>     \"command\": [\n    # >>>         \"sh\",\n    # >>>         \"-c\",\n    # >>>         \"cd /path/to/container && python docker_example.py\"\n    # >>>     ]\n    # >>>  }\n    # >>> 2023-12-29 15:08:58,707 - INFO - easier-docker ==> Find docker image: [python:3.9] locally...\n    # >>> 2023-12-29 15:08:58,724 - INFO - easier-docker ==> Image: [python:3.9] is found locally\n    # >>> 2023-12-29 15:08:58,725 - INFO - easier-docker ==> Find docker container: [python_test] locally...\n    # >>> 2023-12-29 15:08:58,730 - INFO - easier-docker ==> ContainerNotFound: [python_test], it will be created\n    # >>> 2023-12-29 15:09:00,989 - INFO - easier-docker ==> Container name: [python_test] is running\n    # >>> 2023-12-29 15:09:00,990 - INFO - easier-docker ==> Container id: [a9b642f2ddf3] is running\n    # >>> 2023-12-29 15:09:00,990 - INFO - easier-docker ==> Container ip address: [172.17.0.2]\n    # >>> 2023-12-29 15:09:00,991 - INFO - easier-docker ==> Successfully container is running and be created at 2023-12-29T07:08:58.738605891Z\n\n```",
      "EasyLiterature" => "# EasyLiterature\n**EasyLiterature** is a Python-based command line tool for automatic literature management. Welcome star or contribute!\n\nSimply list the paper titles (or ids) you want to read in a markdown file and it will automatically `collect and refine its information in the markdown file`, `download the pdf to your local machine`, and `link the pdf to your paper in the markdown file`. You can forever keep your notes within the pdfs and mds on your local machine or cloud driver.\n\nInspired by [Mu Li](https://www.bilibili.com/video/BV1nA41157y4), adapted from [autoLiterature](https://github.com/wilmerwang/autoLiterature). \nCompared to autoLiterature, **EasyLiterature** is much easier to use and supports a wider range of features, such as `title-based paper match`, `paper search and download on Google Scholar and DBLP` (the two main sites for scholars), `citation statistics`, `mannual information update assitant`, etc. **EasyLiterature covers almost all papers thanks to the support of Google Scholar and DBLP!**\n\n## A simple example\n1. Have the python installed on your local machine (preferably >= 3.7).\n2. Run `pip install easyliter` in your command line to install.\n3. Prepare your markdown note file (e.g., `Note.md`). <br>**Attention:** You may need to download a markdown editor to create/edit this file. I am using [Typora](https://typora.io/), which is not totally free. You can also choose other alternatives.\n4. List the formated papers titles in your markdown note file according to the Section 4 below (Recognition Rules). e.g.,<br>\n  \\- {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.}}<br>\n  \\- {{Xlnet: Generalized autoregressive pretraining for language understanding.}}<br>\n  **(pay attention to the space after \u2018\\-\u2019)** \n5. Create a folder to store the downloaded pdfs (e.g., `PDFs/`).\n6. Run `easyliter -i <path to your md file> -o <path to your pdf folder>`. \n<br> (Replace `<path to your md file>` with the actual path to your markdown note file, `<path to your pdf folder>` with the actual path to your pdf folder)\n<br>e.g., `easyliter -i \"/home/Note.md\" -o \"/home/PDFs\"`\n7. Your should able to see that the updated information and downloaded pdf files if no error is reported.\n8. This is a simple and common use case. For other features, please read the below sections carefully and follow the instructions.\n\n## Arguments\n```bash\neasyliter\n\noptional arguments:\n\n  -h, --help            show this help message and exit\n  \n  -i INPUT, --input INPUT\n  The path to the note file or note file folder.\n\n  -o OUTPUT, --output OUTPUT\n  Folder path to save paper pdfs and images. NOTE: MUST BE FOLDER.\n\n  -p PROXY, --proxy PROXY\n  The proxy. e.g. 127.0.0.1:1080. If this argument is specified, the google scholar will automatically use a free proxy (not necessarily using the specified proxy address). To use other proxies for google scholar, specify the -gp option. If you want to set up the proxies mannually, change the behaviour in GoogleScholar.set_proxy(). See more at https://scholarly.readthedocs.io/en/stable/ProxyGenerator.html.\n\n  -gp GPROXY_MODE, --gproxy_mode GPROXY_MODE\n  The proxy type used for scholarly. e.g., free, single, Scraper. (Note: 1. <free> will automatically choose a free proxy address to use, which is free, but may not be fast. 2. <single> will use the proxy address you specify. 3. <Scraper> is not free to use and need to buy the api key.).\n\n  -d, --delete\n  Delete unreferenced attachments in notes. Use with caution, when used, -i must be a folder path including all notes.\n\n  -m MIGRATION, --migration MIGRATION\n  The pdf folder path you want to reconnect to.\n```\n\n## Recognition Rules\n- If the notes file contains `- {paper_id}`, it will download the information of that literature, but not the PDF.\n- If the notes file contains `- {{paper_id}}`, it will download both the information of that literature and the PDF.\n\n- Note: `paper_id` supports `article title`, published articles' `doi`, and pre-published articles' `arvix_id`, `biorvix_id`, and `medrvix_id`. It will try all the possible sources online.\n\n## Usage\n### Basic Usage\nAssuming `input` is the folder path of the literature notes (.md files) and `output` is the folder path where you want to save the PDFs.\n\n```bash\n# Update all md files in the input folder\neasyliter -i input -o output \n\n# Only update the input/example.md file\neasyliter -i input/example.md -o output  \n\n# -d is an optional flag, when -i is a folder path, using -d will delete unrelated pdf files in the PDF folder from the literature notes content\neasyliter -i input -o output -d\n```\n\n### Migrating Notes and PDF Files\nWhen you need to move the literature notes or the PDF folder, the links to the PDFs in the literature notes might become unusable. You can use `-m` to re-link the PDF files with the literature notes.\n\n```bash\n# Update all md files in the input folder\neasyliter -i input -m movedPDFs/\n\n# Only update the input/example.md file\neasyliter -i input/example.md -m movedPDFs/  \n```",
      "epubhv" => "# epubhv\n\nepubhv is a tool to make your epub books vertical or horizontal or make them readable for language learners.\n\n## Features\n\n- Make your epub books vertical or horizontal\n- Translate your epub books between `\u7b80\u4f53` and `\u7e41\u4f53`\n- Add `ruby` for Japanese(furigana) and Chinese(pinyin)\n- Add `ruby` for `cantonese`\n\n## Using pipx\n\nIf you are using [pipx](https://pypi.org/project/pipx/), you can directly run `epubhv` with:\n\n```console\npipx run epubhv a.epub\n```\n\n## Use the web\n\n```console\npip install epubhv[web]\nstreamlit run web.py\n```\n\n## Use CLI\n\n```console\nepubhv a.epub # will generate a file a-v.epub that is vertical\n# or\nepubhv b.epub --h # will generate a file b-h.epub that is horizontal\n\n# if you also want to translate from `\u7b80\u4f53 -> \u7e41\u4f53`\nepubhv c.epub --convert s2t\n\n# if you also want to translate from `\u7e41\u4f53 -> \u7b80\u4f53`\nepubhv d.epub --h --convert t2s\n\n# or a folder contains butch of epubs\nepubhv tests/test_epub # will generate all epub files to epub-v\n\n# you can specify the punctuation style\nepubhv e.epub --convert s2t --punctuation auto\n# you can add `ruby` for Japanese(furigana) and Chinese(pinyin)\nepubhv e.epub --h --ruby\n# if you want to learn `cantonese` \u7ca4\u8bed\nepubhv f.epub --h --ruby --cantonese\n```",
      "every-breath-you-take" => "# Every Breath You Take \u2013 Heart Rate Variability Training with the Polar H10 Monitor\n\nThrough controlled breathing it is possible to regulate your body's stress reponse. This application allows you to measure and train this effect with a Polar H10 Heart Rate monitor.\n\nHeart rate variability, the small changes in heart rate from beat-to-beat, is a reliable measure of stress response. Heart rate variability reflects the balance between the two sides of the autonomic nervous system: the fight-or-flight response (from the sympathetic nervous system) and the rest-and-digest response (from the parasympathetic nervous system).\n\nIn any moment it is possible to restore balance to the autonomic nervous system by breathing slower and deeper. With every breath you take, you can set the pace of your breathing rate, measure your breathing control with the chest accelerometer, and see how heart rate variability responds.\n\n## Features\n\n- Connect and stream from a Polar H10, acceleration and heart rate data\n- Live breathing control feedback and adjustable pace setting\n- Track breathing and heart rate oscillations in real-time\n- Explore how heart rate vairability repsonses to different breathing rates\n\n## Installation and usage\n\nWorks with Polar H10, with Firmware Version 5.0.0 or later\n    \n    python -m venv venv\n    source venv/bin/activate  # On Windows, use `my_project_env\\Scripts\\activate`\n    pip install -r requirements.txt\n    python EBYT.py \n\nBundle into an application with pyinstaller:\n\n    pyinstaller EBYT.spec\n\nThe program will automatically connect to your Polar device. For best breathing detection, ensure the Polar H10 is fitted around the widest part of the ribcage, stay seated and still while recording.\n\nSet the breathing pace with the slider (in breaths per minute), and follow the cadence as the gold circle expands and contracts. The blue circle shows your breathing control.\n\nTrack each breath cycle in the top graph, and how heart rate oscillates in repsonse.\n\nAdjust breathing pace and control to target the green zone of heart rate variability in the bottom graph (> 150 ms).",
      "EVM_inscription" => "# EVM Inscription\n\n## Overview\n`EVM_inscription` is a Python script tailored for the efficient batch creation of inscriptions on Ethereum Virtual Machine (EVM)-compatible blockchains. It streamlines the process of connecting to various blockchain networks and automates the submission of multiple transactions.\n\n## Features\n- **Multiple Blockchain Support**: Compatible with several EVM blockchains like Ethereum, Binance Smart Chain, and Polygon.\n- **Dynamic Configuration**: Customize the number of inscriptions, transaction data, and gas pricing.\n- **Secure Private Key Handling**: Ensures the safe usage of your private key without hardcoding it in the script.\n- **User-Friendly**: Simple setup and execution process.\n\n## Prerequisites\nBefore using this script, make sure you have Python installed on your system. You can download it from [here](https://www.python.org/downloads/).\n\n## Setup\n1. **Private Key**: For security reasons, do not hardcode your private key in the script. Instead, use an environment variable or a secure key management system.\n2. **RPC URL**: Choose the appropriate RPC URL for the blockchain you intend to interact with.\n3. **Configuration**:\n   - `no_to_mint`: Set the number of inscriptions you wish to create.\n   - `hex_data`: Provide the hexadecimal data for the inscription.\n   - `price_factor`: Adjust the gas price factor according to network conditions.\n\n## Usage\n1. Open the script and enter the required configuration parameters.\n2. Run the script from your terminal:\n   ```\n   python evm_inscription.py\n   ```",
      "fastui-chat" => "# fastui-chat\n\nA minimalistic ChatBot Interface in pure python. </br>\nBuild on top of [FastUI](https://github.com/pydantic/FastUI) and [LangChain Core](https://github.com/langchain-ai/langchain).\n\n## Usage\n\n```bash\npip install fastui-chat\n```\n\n```python\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.memory import ChatMessageHistory\n\nfrom fastui_chat import ChatUI, basic_chat_handler\n\nhistory = ChatMessageHistory()\nhandler = basic_chat_handler(\n    llm=ChatOpenAI(),\n    chat_history=history,\n)\n\nhistory.add_ai_message(\"How can I help you today?\")\n\napp = ChatUI(\n    chat_history=history,\n    chat_handler=handler,\n)\n\napp.start_with_uvicorn()\n```\n\n## Features\n\n- Easy to use\n- Minimalistic & Lightweight\n- LangChain Compatible\n- Python Only",
      "kanban-python" => "# kanban-python\n\n> A Terminal Kanban Application written in Python to boost your productivity\n\n## Introduction\nWelcome to **kanban-python**, your Terminal Kanban-Board Manager.\n\nThe [clikan] Kanban App inspired me to write\nmy own Terminal Kanban Application since I preferred a more simple and guided workflow.\n\n**kanban-python** also comes with more features, like custom column creation,\nautomatic scanning and customizable config file to support you being productive.\n\nThis package was developed with [pyscaffold], which provides awesome project templates\nand takes over much of the boilerplate for python packaging.\nIt was a great help for developing my first package and I can highly recommend it.\n\n## Features\n<details><summary>Colorful and Interactive</summary>\n\n- kanban-python uses [rich] under the hood to process user input\nand display nice looking kanban-boards to the terminal.\n- Each task has a unique `ID` per board and also has an optional `Tag` and `Due Date` associated with it,\nwhich are displayed alongside its `Title`\n\n</details>\n\n\n<details><summary>Following the XDG basedir convention</summary>\n\n- kanban-python utilizes [platformdirs] `user_config_dir` to save the config file and `user_data_dir` for\nthe board specific task files. After creating your first board, you can use `kanban configure` to show the current settings table.\nThe config path in the table caption and the path for the task files can be found in the kanban_boards section.\n\n</details>\n\n\n<details><summary>Scanning of Files for automatic Task Creation</summary>\n\n- kanban-python can scan files of defined types for specific patterns at start of line.\nCheck [Automatic Task Creation](#automatic-task-creation) for more Infos.\n\n</details>\n\n\n<details><summary>Customizable Configfile</summary>\n\n- A `pykanban.ini` file gets created on first initialization in a `kanban-python` folder in your `user_config_dir`-Directory.\nThis can be edited manually or within the kanban-python application. It tracks the location for all your created boards. \\\n![configfile](https://raw.githubusercontent.com/Zaloog/kanban-python/main/images/image_config.PNG)\n   * `Active_Board`: current board that is shown when using `kanban`-command\n   * `Done_Limit`: If the amount of tasks exceed this number in the  <span style=\"color:green\">Done</span> column,\n   the first task of that column gets its status updated to <span style=\"color:gold\">Archived</span> and is moved into that column. (default: `10`)\n   * `Column_Min_Width`: Sets the minimum width of columns. (default: `40`)\n   * `Show_Footer`: Shows the table footer with package name and version. (default: `True`)\n   * `Files`: Space seperated filetypes to search for patterns to create tasks. (default: `.py .md`)\n   * `Patterns`: Comma seperated patterns to search for start of line to create tasks. <br />(default: `# TODO,#TODO,# BUG`)\n\n</details>\n\n\n<details><summary>Task Storage File for each Board</summary>\n\n- Each created board comes with its own name and `pykanban.json` file,\nwhich stores all tasks for that board. The files are stored in board specific folders under `$USER_DATA_DIR/kanban-python/kanban_boards/<BOARDNAME>`.\nWhen changing Boards you also get an overview over tasks in visible columns for each board and the most urgent or overdue task on that board.\n![change_view](https://raw.githubusercontent.com/Zaloog/kanban-python/main/images/image_kanban_change.PNG)\n\n</details>\n\n\n<details><summary>Customizable Columns</summary>\n\n- kanban-python comes with 5 pre-defined colored columns: [Ready, Doing, Done, Archived, Deleted]\nMore column can be added manually in the `pykanban.ini`, the visibility can be configured in the settings\nwith `kanban configure`.\n\n</details>\n\n\n<details><summary>Time Tracking of Task duration in Doing</summary>\n\n- For each task it is tracked, how long it was in the\n <span style=\"color:yellow\">Doing</span> column, based on the moments when you update the task status.\n The initial Task structure on creation looks as follows:\n![task](https://raw.githubusercontent.com/Zaloog/kanban-python/main/images/image_task_example.PNG)\n\n</details>\n\n\n<details><summary>Report Creation for completed Tasks</summary>\n\n- When you use [kanban report](#create-report) a github-like contribution map is displayed for the current year,\nAlso a markdown file is created with all tasks comleted based on the moment, when the tasks were moved to Done Column.\n![task](https://raw.githubusercontent.com/Zaloog/kanban-python/main/images/image_kanban_report_document.PNG)\n\n</details>\n\n## Usage\nAfter Installation of kanban-python, there are 5 commands available:\n\n### Create new Boards\n  ```bash\n  kanban init\n  ```\nIs used to create a new kanban board i.e. it asks for a name and then creates a `pykanban.json` file with a Welcome Task.\nOn first use of any command, the `pykanban.ini` configfile and the `kanban-python` folder will be created automatically.\n\n### Interact with Tasks/Boards\n  ```bash\n  kanban\n  ```\nThis is your main command to interact with your boards and tasks. It also gives the option to show the current settings and adjust them.\nAdjusting the settings can also be done directly by using the command `kanban configure`.\n\nUse `Ctrl-C` or `Ctrl-D` to exit the application at any time. :warning: If you exit in the middle of creating/updating a task,\nor changing settings, your progress wont be saved.\n\n### Automatic Task Creation\n  ```bash\n  kanban scan\n  ```\nAfter executing this command, kanban-python scans your current Directory recursively for the defined filetypes and searches for lines that start with the pattern provided.\n\nAfter confirmation to add the found tasks to table they will be added to the board. The alphanumeric Part of the Pattern will be used as tag.\nThe filepath were the task was found will be added as description of the task.\n\n### Create Report\n  ```bash\n  kanban report\n  ```\nGoes over all your Boards and creates a single markdown file by checking the `Completion Dates` of your tasks.\nAlso shows a nice github-like contribution table for the current year.\n\n### Change Settings\n  ```bash\n  kanban configure\n  ```\nTo create a new custom Columns, you have to edit the `pykanban.ini` manually and add a new column name + visibility status\nunder the `settings.columns.visible` section. The other options are all customizable now via the new settings menu.",
      "flameshow" => "# Flameshow\n\nFlameshow is a terminal Flamegraph viewer.\n\n## Features\n\n- Renders Flamegraphs in your terminal\n- Supports zooming in and displaying percentages\n- Keyboard input is prioritized\n- All operations can also be performed using the mouse.\n- Can switch to different sample types\n\n## Usage\n\nView golang's goroutine dump:\n\n```shell\n$ curl http://localhost:9100/debug/pprof/goroutine -o goroutine.out\n$ flameshow goroutine.out\n```\n\nAfter entering the TUI, the available actions are listed on Footer:\n\n- <kbd>q</kbd> for quit\n- <kbd>j</kbd> <kbd>i</kbd> <kbd>j</kbd> <kbd>k</kbd> or <kbd>\u2190</kbd>\n  <kbd>\u2193</kbd> <kbd>\u2191</kbd> <kbd>\u2192</kbd> for moving around, and <kbd>Enter</kbd>\n  for zoom in, then <kbd>Esc</kbd> for zoom out.\n- You can also use a mouse, hover on a span will show it details, and click will\n  zoom it.\n\n## Supported Formats\n\nAs far as I know, there is no standard specification for profiles. Different\nlanguages or tools might generate varying profile formats. I'm actively working\non supporting more formats. Admittedly, I might not be familiar with every tool\nand its specific format. So, if you'd like Flameshow to integrate with a tool\nyou love, please feel free to reach out and submit an issue.\n\n- Golang pprof\n- [Brendan Gregg's Flamegraph](https://www.brendangregg.com/flamegraphs.html)",
      "libgen_to_txt" => "# Libgen to txt\n\nThis repo will convert books from libgen to plain txt or markdown format.  This repo does not contain any books, only the scripts to download and convert them.\n\nThe scripts use a seedbox to download the libgen torrents, copy them to your machine/cloud instance, convert them, and enrich them with metadata.  Processing will be by chunk, with configurable parallelization.\n\nIt currently only works for the libgen rs nonfiction section, but PRs welcome for additional compatibility.  It will cost about $300 to convert all of libgen rs nonfiction if you're using a cloud instance, and take about 1 week to process everything (bandwidth-bound).  You'll need 3TB of disk space.\n\n# Configuration\n\n- Get a putio oauth token following [these instructions](https://help.put.io/en/articles/5972538-how-to-get-an-oauth-token-from-put-io)\n- Either set the env var `PUTIO_TOKEN`, or create a `local.env` file with `PUTIO_TOKEN=yourtoken`\n- Inspect `libgen_to_txt/settings.py`.  You can edit settings directly to override them, set an env var, or add the key to a `local.env` file.\n  - You may particularly want to look at `CONVERSION_WORKERS` and `DOWNLOAD_WORKERS` to control parallelization.  The download step is the limiting factor, and too many download workers will saturate your bandwidth.\n\n# Usage\n\n- `python download_and_clean.py` to download and clean the data\n  - `--workers` to control number of download workers (how many parallel downloads happen at once)\n  - `--no_download` to only process libgen chunks that already exist on the seedbox\n  - `--max` controls how many chunks at most to process (for testing)\n  - `--no_local_delete` to avoid deleting chunks locally after they're downloaded.  Mainly useful for debugging.\n\nYou should see progress information printed out - it will take several weeks to finish depending on bandwidth and conversion method (see below).  Check the `txt` and `processed` folders to monitor.\n\n## Markdown conversion\n\nThis can optionally be integrated with [marker](https://www.github.com/VikParuchuri/marker) to do high-accuracy pdf to markdown conversion.  To use marker, first install it, then:\n\n- `CONVERSION_METHOD` to `marker`\n- `MARKER_FOLDER` to the path to the marker folder\n\n`CONVERSION_WORKERS` will control how many marker processes per GPU are run in parallel.  Marker takes about 2.5GB of VRAM per process, so set this accordingly.\n\nYou can adjust additional settings around how marker is integrated using the `MARKER_*` settings.  In particular, pay attention to the timeouts.  These ensure that conversion doesn't get stuck on a chunk. Marker can run on CPU or GPU, but is much faster on GPU.  With 4x GPUs, a single libgen chunk should take about 1 hour to process.\n\n# Cloud storage\n\nYou can store the converted txt/markdown files in a s3-compatible storage backend as they're processed using `s3fs`.  Here's how:\n\n- `sudo apt install s3fs`\n- `echo ACCESS_KEY_ID:SECRET_ACCESS_KEY > ${HOME}/.passwd-s3fs`\n- `chmod 600 ${HOME}/.passwd-s3fs`\n- `s3fs BUCKET_NAME LOCAL_DIR -o url=STORAGE_URL -o use_cache=/tmp -o allow_other -o use_path_request_style -o uid=1000 -o gid=1000 -o passwd_file=${HOME}/.passwd-s3fs`",
    }
    repo_name = "CVE-2023-44487"
    readme_content = Map.get(readme_map, repo_name)

    actor_specs = generate_actor_specs()
    IO.inspect(actor_specs)
    {:ok, _supervisor} = Ain.ActorSupervisor.start_link(actor_specs)

    # Allow some time for the actors to start
    :timer.sleep(1_000)

    actor_pids = for %{uid: uid} <- actor_specs do
      pid = GenServer.whereis({:global, uid})
      {uid, pid}
    end

    {reposketcher_uuid, reposketcher_pid} = Enum.at(actor_pids, 0)
    file_sketcher_pids = Enum.slice(actor_pids, 1, 3)
    sketch_fillers = Enum.slice(actor_pids, 4, 6)
    {packer_uid, packer_pid} = Enum.at(actor_pids, 7)
    {evaluator_uid, evaluator_pid} = Enum.at(actor_pids, 8)
    for {file_sketcher_uuid, file_sketcher_pid} <- file_sketcher_pids do
      connect_actors(reposketcher_pid, file_sketcher_pid, file_sketcher_uuid, actor_specs)
    end

    # 连接每个 file_sketcher 到每个 sketch_filler
    for {file_sketcher_uuid, file_sketcher_pid} <- file_sketcher_pids do
      for {sketch_filler_uuid, sketch_filler_pid} <- sketch_fillers do
        connect_actors(file_sketcher_pid, sketch_filler_pid, sketch_filler_uuid, actor_specs)
      end
    end

    connect_actors(packer_pid, evaluator_pid, evaluator_uid, actor_specs)

    message = %Message{
      content: readme_content,
      parameters: %{
        "from_role": ""
      },
    }
    GenServer.cast(reposketcher_pid, {:receive, message})
    :timer.sleep(120_000)
    message = %Message{
      content: repo_name,
      parameters: %{
        "from_role": ""
      },
    }
    GenServer.cast(packer_pid, {:receive, message})
  end

  defp generate_actor_specs do
    repo_sketcher = %{uid: "repo_sketcher", name: "RepoSketcher", role: "RepoSketcher"}
    file_sketchers = for i <- 1..3, do: %{uid: "file_sketcher_#{i}", name: "FileSketcher_#{i}", role: "FileSketcher"}
    sketch_fillers = for i <- 1..3, do: %{uid: "sketch_filler_#{i}", name: "SketchFiller_#{i}", role: "SketchFiller"}
    packer = %{uid: "packer", name: "Packer", role: "Packer"}
    evaluator = %{uid: "evaluator", name: "Evaluator", role: "Evaluator"}
    [repo_sketcher | file_sketchers ++ sketch_fillers ++ [packer, evaluator]]
  end

  defp connect_actors(from_pid, to_pid, to_uid, actor_specs) do
    to_role = get_role(to_uid, actor_specs)
    message = %Message{
      content: "hi",
      parameters: %{
        "to_uid" => to_uid,
        "to_pid" => to_pid,
        "to_role" => to_role
      },
    }
    GenServer.cast(from_pid, {:explore, message})
  end

  defp get_role(uuid, actor_specs) do
    actor_specs
    |> Enum.find(fn actor_spec -> actor_spec.uid == uuid end)
    |> Map.get(:role)
  end
end
